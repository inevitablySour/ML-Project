{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "drider = pd.read_csv(r'.\\data\\rider_infos.csv')\n",
    "\n",
    "drace = pd.read_csv(r'.\\data\\results\\Results2017CatWT.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather previous results\n",
    "from os import listdir\n",
    "from os.path import isfile, join, split\n",
    "from pathlib import Path\n",
    "\n",
    "mypath = r\".\\data\\results\"\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "# data van 2012 t/m 2021 --> neem 3 jaar in het verleden  mee, dus van 2016 t/m 2021 --> index 3 t/m 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit is code om de CSV's naar SQLite te converteren, verder niet van belang... De studenten krijgen de SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\results\\Results2012CatWT.csv\n",
      ".\\data\\results\\Results2014CatWT.csv\n",
      ".\\data\\results\\Results2015CatWT.csv\n",
      ".\\data\\results\\Results2016CatWT.csv\n",
      ".\\data\\results\\Results2017CatWT.csv\n",
      ".\\data\\results\\Results2018CatWT.csv\n",
      ".\\data\\results\\Results2019CatWT.csv\n",
      ".\\data\\results\\Results2020CatWT.csv\n",
      ".\\data\\results\\Results2021CatWT.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import hashlib\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define database file name\n",
    "db_file = \"cycling_big.db\"\n",
    "\n",
    "# Remove the existing database file if it exists\n",
    "if os.path.exists(db_file):\n",
    "    os.remove(db_file)\n",
    "\n",
    "# Connect to SQLite database (this will create a new, clean file)\n",
    "conn = sqlite3.connect(db_file)\n",
    "# Enable foreign key support\n",
    "conn.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "\n",
    "# Step 1: Create tables with the correct schema and foreign key constraints\n",
    "conn.execute(\"\"\"\n",
    "CREATE TABLE riders (\n",
    "    rider_id TEXT PRIMARY KEY,\n",
    "    fullname TEXT NOT NULL,\n",
    "    team TEXT,\n",
    "    birthdate TEXT NOT NULL,\n",
    "    country TEXT,\n",
    "    height REAL,\n",
    "    weight REAL,\n",
    "    rider_url TEXT,\n",
    "    pps TEXT,\n",
    "    rdr TEXT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "conn.execute(\"\"\"\n",
    "CREATE TABLE race_results (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    Rnk TEXT,\n",
    "    GC REAL,\n",
    "    Timelag TEXT,\n",
    "    BiB TEXT,\n",
    "    Rider TEXT,\n",
    "    Age INTEGER,\n",
    "    Team TEXT,\n",
    "    UCI REAL,\n",
    "    Pnt REAL,\n",
    "    Time TEXT,\n",
    "    Circuit INTEGER,\n",
    "    Race_Name TEXT,\n",
    "    Stage_Name TEXT,\n",
    "    Date TEXT,\n",
    "    Stage_Type TEXT,\n",
    "    Start TEXT,\n",
    "    Finish TEXT,\n",
    "    Race_ID INTEGER,\n",
    "    Stage_Number INTEGER,\n",
    "    Length TEXT,\n",
    "    Category TEXT,\n",
    "    Race_url TEXT,\n",
    "    Stage_url TEXT,\n",
    "    rider_id TEXT,\n",
    "    FOREIGN KEY (rider_id) REFERENCES riders(rider_id) ON DELETE SET NULL\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Load your datasets\n",
    "riders = pd.read_csv(r'.\\data\\rider_infos.csv')\n",
    "# Step 1: Create a unique rider_id in the riders dataset\n",
    "def generate_rider_id(row):\n",
    "    unique_string = f\"{row['fullname']}_{row['birthdate']}\"\n",
    "    return hashlib.md5(unique_string.encode()).hexdigest()\n",
    "\n",
    "riders['rider_id'] = riders.apply(generate_rider_id, axis=1)\n",
    "riders = riders.drop(columns=['Unnamed: 0'])\n",
    "riders.to_sql(\"riders\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# race_results = pd.read_csv(r'.\\data\\results\\Results2017CatWT.csv')\n",
    "# print(riders.columns)\n",
    "# print(riders.dtypes)\n",
    "\n",
    "# print(race_results.columns)\n",
    "# print(race_results.dtypes)\n",
    "\n",
    "for i in range(len(onlyfiles)): #range(3,9):\n",
    "    f = r'.\\data\\results\\{}'.format(onlyfiles[i])\n",
    "    print(f)\n",
    "    race_results = pd.read_csv(f)\n",
    "    # Step 2: Map rider_id to the race results dataset\n",
    "    # Ensure 'Rider' in race_results matches 'fullname' in riders\n",
    "    race_results = race_results.merge(\n",
    "        riders[['fullname', 'rider_id']],\n",
    "        left_on='Rider',\n",
    "        right_on='fullname',\n",
    "        how='left'\n",
    "    )\n",
    "    # Remove the redundant 'fullname' column added during the merge\n",
    "    race_results.drop(columns=['fullname'], inplace=True)\n",
    "\n",
    "    # Step 2: Insert data into the tables\n",
    "    race_results = race_results.drop(columns=['Unnamed: 0'])\n",
    "    race_results.rename(columns={'Stage#': 'Stage_Number'}, inplace=True)\n",
    "    # Write to SQL\n",
    "\n",
    "    race_results.to_sql(\"race_results\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit is code iom de velder pps en rdr die json achtige subdata bevatten om te zetten naar nieuwe colommen. Dit wordt nog niet teogepast hieronder voor de ML dataset... TODO i guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0_x', 'Rnk', 'GC_x', 'Timelag', 'BiB', 'Rider', 'Age', 'Team',\n",
      "       'UCI', 'Pnt', 'Time', 'Circuit', 'Race_Name', 'Stage_Name', 'Date',\n",
      "       'Stage_Type', 'Start', 'Finish', 'Race_ID', 'Stage#', 'Length',\n",
      "       'Category', 'Race_url', 'Stage_url', 'Unnamed: 0_y', 'fullname', 'team',\n",
      "       'birthdate', 'country', 'height', 'weight', 'rider_url', 'pps', 'rdr',\n",
      "       'One day races', 'GC_y', 'Time trial', 'Sprint', 'Climber',\n",
      "       'PCS Ranking', 'UCI World Ranking', 'Specials | All Time Ranking'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "tojson = lambda x: json.loads(x)\n",
    "replace = lambda x: x.replace(\"'\",\"\\\"\")\n",
    "\n",
    "pps = drider[\"pps\"].apply(replace)\n",
    "arecords = pps.apply(tojson).to_list()\n",
    "a = pd.DataFrame.from_dict(arecords)\n",
    "dnew = pd.concat((drider,a),axis=1)\n",
    "\n",
    "rdr = drider[\"rdr\"].apply(replace)\n",
    "arecords = rdr.apply(tojson).to_list()\n",
    "a = pd.DataFrame.from_dict(arecords)\n",
    "dnew = pd.concat((dnew,a),axis=1)\n",
    "\n",
    "\n",
    "d_all = drace.merge(right=dnew, left_on=\"Rider\", right_on=\"fullname\")\n",
    "print(d_all.columns)\n",
    "d_all.drop(columns=['Unnamed: 0_x', 'Time', 'BiB', \n",
    "       'Start', 'Finish', 'Race_ID', 'Stage#', 'Race_url', \n",
    "       'Stage_url', 'Unnamed: 0_y', 'rider_url', 'pps', 'rdr'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit is de code om de historische data te transformeren van een tabel met rider/etappe/race uitslagen, naar iets dat gaat over de plaatsing van een rider over de afgelopen n jaren. Deze maakt nu gebruikt van het veld \"Pnt\" in de originele data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\\data\\results\\Results2016CatWT.csv\n",
      "   .\\data\\results\\Results2015CatWT.csv\n",
      "   .\\data\\results\\Results2014CatWT.csv\n",
      "   .\\data\\results\\Results2012CatWT.csv\n",
      ".\\data\\results\\Results2017CatWT.csv\n",
      "   .\\data\\results\\Results2016CatWT.csv\n",
      "   .\\data\\results\\Results2015CatWT.csv\n",
      "   .\\data\\results\\Results2014CatWT.csv\n",
      ".\\data\\results\\Results2018CatWT.csv\n",
      "   .\\data\\results\\Results2017CatWT.csv\n",
      "   .\\data\\results\\Results2016CatWT.csv\n",
      "   .\\data\\results\\Results2015CatWT.csv\n",
      ".\\data\\results\\Results2019CatWT.csv\n",
      "   .\\data\\results\\Results2018CatWT.csv\n",
      "   .\\data\\results\\Results2017CatWT.csv\n",
      "   .\\data\\results\\Results2016CatWT.csv\n",
      ".\\data\\results\\Results2020CatWT.csv\n",
      "   .\\data\\results\\Results2019CatWT.csv\n",
      "   .\\data\\results\\Results2018CatWT.csv\n",
      "   .\\data\\results\\Results2017CatWT.csv\n",
      ".\\data\\results\\Results2021CatWT.csv\n",
      "   .\\data\\results\\Results2020CatWT.csv\n",
      "   .\\data\\results\\Results2019CatWT.csv\n",
      "   .\\data\\results\\Results2018CatWT.csv\n"
     ]
    }
   ],
   "source": [
    "frames=[]\n",
    "for i in range(3,9): #range(3,9):\n",
    "    f = r'.\\data\\results\\{}'.format(onlyfiles[i])\n",
    "    print(f)\n",
    "    drace = pd.read_csv(f)\n",
    "    drace = drace.merge(right=dnew, left_on=\"Rider\", right_on=\"fullname\")\n",
    "    #print(i)\n",
    "    for j in range(1,4):\n",
    "        idx = i-j\n",
    "        f = r'.\\data\\results\\{}'.format(onlyfiles[idx])\n",
    "        print('   '+f)\n",
    "        d3 = pd.read_csv(f)\n",
    "        gr = d3.groupby([\"Race_Name\",\"Rider\"])\n",
    "        gr = gr[\"Pnt\"].sum()\n",
    "        race_rider = gr.unstack(level=0)\n",
    "        race_rider[\"Sum\"] = race_rider.sum(axis=1, skipna=True)\n",
    "        \n",
    "        totalpoints = race_rider.pop(\"Sum\")\n",
    "        \n",
    "        drace = drace.merge(right=totalpoints, left_on=\"Rider\", right_index=True)\n",
    "        \n",
    "        #col_name = Path(f).stem\n",
    "        col_name = 'sumres_'+str(j)\n",
    "        \n",
    "        drace = drace.rename(columns = {\"Sum\" : col_name})\n",
    "    #race_rider.head(15)\n",
    "    frames.append(drace)\n",
    "\n",
    "drace = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dit is code voor variable selectie en transformatie (category naar one-hot etc..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnk                            float16\n",
      "GC_x                           float16\n",
      "Age                            float16\n",
      "Pnt                            float16\n",
      "Length                         float16\n",
      "height                         float16\n",
      "weight                         float16\n",
      "One day races                  float16\n",
      "GC_y                           float16\n",
      "Time trial                     float16\n",
      "Sprint                         float16\n",
      "Climber                        float16\n",
      "PCS Ranking                    float16\n",
      "UCI World Ranking              float16\n",
      "Specials | All Time Ranking    float16\n",
      "sumres_1                       float16\n",
      "sumres_2                       float16\n",
      "sumres_3                       float16\n",
      "Stage_Type_RR                  float16\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "c = drace.columns\n",
    "#print(c)\n",
    "sel = [\"Rnk\", \"GC_x\", \"Age\", \"Pnt\", \"Stage_Type\", \"Length\", 'height', 'weight', 'One day races', 'GC_y', 'Time trial', 'Sprint', 'Climber',\n",
    "       'PCS Ranking', 'UCI World Ranking', 'Specials | All Time Ranking',\n",
    "       'sumres_1', 'sumres_2', 'sumres_3']\n",
    "\n",
    "sel = [\"Rnk\", \"GC_x\", \"Age\", \"Pnt\", \"Stage_Type\", \"Length\", 'height', 'weight', 'One day races', 'GC_y', 'Time trial', 'Sprint', 'Climber',\n",
    "       'PCS Ranking', 'UCI World Ranking', 'Specials | All Time Ranking',\n",
    "       'sumres_1', 'sumres_2', 'sumres_3']\n",
    "\n",
    "drace_sel = drace[sel]\n",
    "#print(drace_sel.dtypes)\n",
    "#print(drace_sel[\"Pnt\"][1]+1)\n",
    "pntsel = drace_sel[\"Pnt\"].isna()\n",
    "#drace_sel[\"Rnk\"].replace(to_replace=\"DNF\", value=np.NaN, inplace=True)\n",
    "#drace_sel[\"Rnk\"].replace(to_replace=\"OTL\", value=np.NaN, inplace=True)\n",
    "ss = drace_sel.Rnk.str.isnumeric()\n",
    "drace_sel = drace_sel[drace_sel.Rnk.str.isnumeric()]\n",
    "\n",
    "drace_sel.Stage_Type = drace_sel.Stage_Type.astype(\"category\")\n",
    "drace_sel.Length = drace_sel.Length.apply(lambda x: float(x.split(\" \")[0]))\n",
    "drace_sel = pd.get_dummies(drace_sel, drop_first=True, columns=[\"Stage_Type\"])\n",
    "drace_sel = drace_sel.astype(np.float16, copy=False)\n",
    "print(drace_sel.dtypes)\n",
    "#drace_sel=drace_sel.iloc[not pntsel.to_numpy(),:]\n",
    "#drace_sel.dropna(inplace=True, subset=[\"Pnt\"])\n",
    "\n",
    "#drace_sel_na = drace_sel.dropna()\n",
    "drace_sel = drace_sel.drop(columns=[\"Pnt\"])\n",
    "drace_sel[\"Rnk\"] = drace_sel[\"Rnk\"]<=30 #maak label binair (wel/geen 30 pnt)?\n",
    "drace_sel[\"Rnk\"] = drace_sel[\"Rnk\"].astype(np.int8)\n",
    "\n",
    "drace_sel.dropna(inplace=True)\n",
    "\n",
    "y = drace_sel[\"Rnk\"]\n",
    "\n",
    "X = drace_sel.drop(columns=[\"Rnk\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7928513403736799\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       top x       0.84      0.87      0.85      5935\n",
      "      losers       0.68      0.62      0.65      2682\n",
      "\n",
      "    accuracy                           0.79      8617\n",
      "   macro avg       0.76      0.75      0.75      8617\n",
      "weighted avg       0.79      0.79      0.79      8617\n",
      "\n",
      "[[5159  776]\n",
      " [1009 1673]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#model = LogisticRegression(solver=\"liblinear\", verbose=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression(solver=\"lbfgs\", verbose=0, fit_intercept=True, penalty=\"l2\"))\n",
    "pipe.fit(X_train, y_train)  # apply scaling on training data\n",
    "\n",
    "print(pipe.score(X_test, y_test))\n",
    "\n",
    "#model.fit(X_train, y_train)\n",
    "\n",
    "yhat = pipe.predict(X_test)\n",
    "phat = pipe.predict_proba(X_test)\n",
    "\n",
    "acc =  accuracy_score(y_test, yhat)\n",
    "\n",
    "cmat = confusion_matrix(y_test, yhat)\n",
    "\n",
    "print(classification_report(y_test, yhat, target_names=[\"top x\", \"losers\"]))\n",
    "\n",
    "print(cmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "drace.dtypes\n",
    "a= drace.Length[0]\n",
    "b= float(a.split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
